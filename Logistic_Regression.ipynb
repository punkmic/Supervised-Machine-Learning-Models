{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/punkmic/Supervised-Machine-Learning-Models/blob/master/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Goals**\n",
        " \n",
        "\n",
        "*   Implement a Logistic Regression Model from scratch.\n",
        "*   Train a logistic regression model using scikit-learn.\n",
        "\n"
      ],
      "metadata": {
        "id": "6vHLRlY76Jtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Logistic Model from scratch**"
      ],
      "metadata": {
        "id": "qhyD4_keQyZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import math"
      ],
      "metadata": {
        "id": "Z-9g9bMd5r2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sigmoid Function: since the output y is either 0 or 1 we need a function that can map all input values to values between 0 and 1.**"
      ],
      "metadata": {
        "id": "nM6uEEEh5rg3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE1O04A75nWa"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  g = 1/(1+np.exp(-z))\n",
        "  return g"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cost function**"
      ],
      "metadata": {
        "id": "ql2z4hzR7WRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic(X, y, w, b):\n",
        "  m = X.shape[0]\n",
        "  cost = 0.0\n",
        "  for i in range(m):\n",
        "    z_i = np.dot(X[i], w) + b\n",
        "    f_wb_i = sigmoid(z_i)\n",
        "    cost += -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
        "  cost = cost / m\n",
        "  return cost"
      ],
      "metadata": {
        "id": "VzwXpVSn7cJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp = np.array([1,1])\n",
        "b_tmp = -3\n",
        "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-q88s9L8cfy",
        "outputId": "67669281-7213-4be2-b272-3fc0fd9f5f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.36686678640551745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compute gradient logistic**"
      ],
      "metadata": {
        "id": "SgmvhjG9BYhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic(X, y, w, b):\n",
        "  m, n = X.shape\n",
        "  dj_dw = np.zeros((n,))\n",
        "  dj_db = 0.\n",
        "\n",
        "  for i in range(m): # For each example (m) - row \n",
        "    z = np.dot(X[i], w) + b  # Note that np.dot(x0*w0 + x1*w1 + x2*w2 ...) + b\n",
        "    f_wb_i = sigmoid(z) # Calculate the sigmoid that ranges from 0 to 1\n",
        "    err_i = f_wb_i - y[i]  # Calculate the difference between the prediction and the actual value 0 or 1 for classification.\n",
        "    for j in range(n):  # For each atribute (n) - column\n",
        "      dj_dw[j] = dj_dw[j] + err_i * X[i, j]  \n",
        "    dj_db = dj_db + err_i\n",
        "  dj_dw = dj_dw/m                                  \n",
        "  dj_db = dj_db/m                                   \n",
        "        \n",
        "  return dj_db, dj_dw\n",
        "\n"
      ],
      "metadata": {
        "id": "Zt60MYQxBfXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
        "w_tmp = np.array([2.,3.])\n",
        "b_tmp = 1.\n",
        "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
        "m, n = X_tmp.shape\n",
        "print(f\"Shape: rows({m}) - columns({n})\")\n",
        "print(f\"dj_db: {dj_db_tmp}\" )\n",
        "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
      ],
      "metadata": {
        "id": "_HEbiOhmG4og",
        "outputId": "9fa0ad04-fa19-421e-cd20-c66321444a10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: rows(6) - columns(2)\n",
            "dj_db: 0.49861806546328574\n",
            "dj_dw: [0.498333393278696, 0.49883942983996693]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient descent**"
      ],
      "metadata": {
        "id": "ueA4tCpCHpvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               \n",
        "        b = b - alpha * dj_db               \n",
        "      \n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion \n",
        "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
        "        \n",
        "    return w, b, J_history         #return final w,b and J history for graphing\n"
      ],
      "metadata": {
        "id": "Gu4fXXq9HspV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp  = np.zeros_like(X_train[0])\n",
        "b_tmp  = 0.\n",
        "alph = 0.1\n",
        "iters = 10000\n",
        "\n",
        "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
        "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
      ],
      "metadata": {
        "id": "0hpLwubQID_x",
        "outputId": "b761bbdb-766e-4b5a-d53b-40b13504db3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost 0.684610468560574   \n",
            "Iteration 1000: Cost 0.1590977666870457   \n",
            "Iteration 2000: Cost 0.08460064176930078   \n",
            "Iteration 3000: Cost 0.05705327279402531   \n",
            "Iteration 4000: Cost 0.04290759421682   \n",
            "Iteration 5000: Cost 0.03433847729884557   \n",
            "Iteration 6000: Cost 0.02860379802212006   \n",
            "Iteration 7000: Cost 0.02450156960879306   \n",
            "Iteration 8000: Cost 0.02142370332569295   \n",
            "Iteration 9000: Cost 0.019030137124109114   \n",
            "\n",
            "updated parameters: w:[5.28123029 5.07815608], b:-14.222409982019837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predict**"
      ],
      "metadata": {
        "id": "OCOTZNgcrJnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b): \n",
        "    \"\"\"\n",
        "    Predict whether the label is 0 or 1 using learned logistic\n",
        "    regression parameters w\n",
        "    \n",
        "    Args:\n",
        "    X : (ndarray Shape (m, n))\n",
        "    w : (array_like Shape (n,))      Parameters of the model\n",
        "    b : (scalar, float)              Parameter of the model\n",
        "\n",
        "    Returns:\n",
        "    p: (ndarray (m,1))\n",
        "        The predictions for X using a threshold at 0.5\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m, n = X.shape   \n",
        "    p = np.zeros(m)\n",
        "   \n",
        "    ### START CODE HERE ### \n",
        "    # Loop over each example\n",
        "    for i in range(m):   \n",
        "        z_wb = 0\n",
        "        # Loop over each feature\n",
        "        for j in range(n): \n",
        "            # Add the corresponding term to z_wb\n",
        "            z_wb += X[i, j] * w[j]\n",
        "        \n",
        "        # Add bias term \n",
        "        z_wb += b\n",
        "        \n",
        "        # Calculate the prediction for this example\n",
        "        f_wb = sigmoid(z_wb)\n",
        "\n",
        "        # Apply the threshold\n",
        "        if f_wb >= 0.5:\n",
        "            p[i] = 1 \n",
        "        else:\n",
        "            p[i] = 0\n",
        "        \n",
        "    ### END CODE HERE ### \n",
        "    return p"
      ],
      "metadata": {
        "id": "QukYS-U1rLX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scikit-learn: Logistic Regression Model**"
      ],
      "metadata": {
        "id": "DSNQ7GRhQ7S0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**"
      ],
      "metadata": {
        "id": "65MlFPloRJlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y = np.array([0, 0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "0YmLXxtFRLJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fit the model**"
      ],
      "metadata": {
        "id": "JxaJIR0QRO97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X, y)"
      ],
      "metadata": {
        "id": "0fFxTg9oRRir",
        "outputId": "519caca7-052e-4dc5-d72f-15fce2c63946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Make predictions**"
      ],
      "metadata": {
        "id": "rhChYg10RUun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr_model.predict(X)\n",
        "\n",
        "print(\"Prediction on training set:\", y_pred)"
      ],
      "metadata": {
        "id": "vf3JRbwJRYYO",
        "outputId": "484a2b86-9c89-4df7-c80e-3e782c4b2314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction on training set: [0 0 0 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculate accuracy**"
      ],
      "metadata": {
        "id": "opPhQcRJRlwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = lr_model.score(X, y)\n",
        "print(\"Accuracy on training set:\", acc)"
      ],
      "metadata": {
        "id": "aiZrEl04RjhU",
        "outputId": "b21a4957-510f-4992-9a42-14819b4e8b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 1.0\n"
          ]
        }
      ]
    }
  ]
}